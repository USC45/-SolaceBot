# -*- coding: utf-8 -*-
"""chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B0zcZp4Y3VTJTp3k7k9TCqmiYuYuf1Rl
"""



import os
from langchain_community.llms import OpenAI, GooglePalm  # Correct import
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import SimpleSequentialChain
from langchain.chains import LLMChain, SequentialChain
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings  # if used
# Update memory and LLMChain usage based on latest syntax


#!pip install langchain openai google-generativeai

from langchain.chat_models import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import ChatGoogleGenerativeAI
#from embeddings import retriever

import getpass
import os

os.environ["GOOGLE_API_KEY"] = "AIzaSyA0p-1w5LWhBH3t2DctVzhGhreq1d3qeZc"

# Now you can use the API key in your code:
api_key = os.environ["GOOGLE_API_KEY"]

# Example of how to use it with a library that requires the key.
import google.generativeai as genai

genai.configure(api_key=api_key)

gemini_llm1 = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)


def is_safe_input(user_input):
    banned_words = ["suicide", "kill myself", "end it all"]  # Expand this list
    for word in banned_words:
        if word.lower() in user_input.lower():
            return False
    return True

from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

summary_prompt = PromptTemplate(
    input_variables=["user_input"],
    template="""
Summarize the following message to identify the user's emotional concern clearly:
{user_input}
Summary:
"""
)

answer_prompt = PromptTemplate(
    input_variables=["summarized_question"],
    template: """
    You are a compassionate mental health support assistant.

Follow these steps:

1. **Check the topic:** If the input is unrelated to mental health, well-being, or casual emotional conversation, respond with something like: 
   _"I'm here to support mental health and emotional well-being. This topic might be outside my scope, but feel free to share how you're feeling."_

2. **Give a response:** If the topic is related, respond with a **single, emotionally supportive** message. Keep it simple (1â€“3 sentences), sound like a caring friend, and offer only one suggestion unless asked.

3. **Help further:** If helpful, suggest one **relevant** mental health exercise or activity, potentially based on recent online information (e.g., mindfulness, journaling, breathing). Keep it short and actionable.

Situation:
{summarized_question}

Response:
"""
)



from langchain.chains import LLMChain, SequentialChain

summary_chain = LLMChain(llm=gemini_llm1, prompt=summary_prompt, output_key="summarized_question")
answer_chain = LLMChain(llm=gemini_llm1, prompt=answer_prompt, output_key="gemini_response")


chatbot_chain = SequentialChain(
    chains=[summary_chain, answer_chain],
    input_variables=["user_input"],
    output_variables=["gemini_response"],
    memory=memory,
    verbose=True
)


class MentalHealthChatbot:
    def __init__(self):
        self.chain = chatbot_chain

    def get_response(self, user_input):
        if not is_safe_input(user_input):
            return "I'm really sorry you're feeling this way. Please consider reaching out to a professional or a trusted person for support."

        # ðŸ§  Use retriever to fetch relevant content
        #docs = retriever.invoke(user_input)

        #context = "\n".join([doc.page_content for doc in docs])

        # ðŸ’¡ You can use context to improve LLM response
        #full_input = f"{user_input}\n\nRelevant Info:\n{context}"

        # Run the chain
        response = self.chain.run({"user_input": user_input})
        return response

